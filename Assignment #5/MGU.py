#!/usr/bin/env python
# coding: utf-8

# In[3]:


# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y_XOqOMBDR7WPRfUC-ykNX4a3AQ0lKU9
"""
import pickle
import os
import numpy as np
import cv2 as cv
import tensorflow as tf
from tensorflow.python.keras.datasets import mnist
from tensorflow.contrib.eager.python import tfe

import matplotlib.pyplot as plt
# enable eager mode
tf.enable_eager_execution()
tf.set_random_seed(0)
np.random.seed(0)

if not os.path.exists('weights/'):
    os.makedirs('weights/')

# constants
units = 128
batch_size = 100
epochs = 3
num_classes = 10
pickle_in = open("./notMNIST.pickle","rb")
dict = pickle.load(pickle_in)
# dataset loading
#
# folder='data'
# from tensorflow.examples.tutorials.mnist import input_data
# (x_train, y_train) =loadlocal_mnist(
#         images_path='./data/train-images-idx3-ubyte',
#         labels_path='./data/train-labels-idx1-ubyte')
# train, val, test=input_data.read_data_sets('./data/not',one_hot="True")
# train, val, test=input_data.read_data_sets('data/fashion',source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/',one_hot="True")
x_train =dict['train_dataset'].astype('float32') / 255.
x_test =dict['test_dataset'].astype('float32') / 255.
x_train = x_train.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timestep
x_test = x_test.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timeste

# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy
# and tensors as input to keras
y_train_ohe =tf.one_hot(dict['train_labels'],depth=num_classes).numpy()
y_test_ohe = tf.one_hot(dict['test_labels'],depth=num_classes).numpy()

print('x train', x_train.shape)
print('y train', y_train_ohe.shape)
print('x test', x_test.shape)
print('y test', y_test_ohe.shape)

import tensorflow as tf


class BasicLSTM(tf.keras.Model):
    def __init__(self, units, return_sequence=False, return_states=False, **kwargs):
        super(BasicLSTM, self).__init__(**kwargs)
        self.units = units
        self.return_sequence = return_sequence
        self.return_states = return_states

        def bias_initializer(_, *args, **kwargs):
            # Unit forget bias from the paper
            # - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)
            return tf.keras.backend.concatenate([
                tf.keras.initializers.Zeros()((self.units,), *args, **kwargs),  # input gate
                tf.keras.initializers.Ones()((self.units,), *args, **kwargs),  # forget gate
                tf.keras.initializers.Zeros()((self.units * 2,), *args, **kwargs),  # context and output gates
            ])

        self.kernel = tf.keras.layers.Dense(4 * units, use_bias=False)
        self.recurrent_kernel = tf.keras.layers.Dense(4 * units, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)

    def call(self, inputs, training=None, mask=None, initial_states=None):
        # LSTM Cell in pure TF Eager code
        # reset the states initially if not provided, else use those
        if initial_states is None:
            # dummy_x = tf.zeros([1, 28, 28])
            #
            # model._set_inputs(dummy_x)
            h_state = tf.zeros((1, self.units))
            c_state = tf.zeros((1, self.units))
        else:
            assert len(initial_states) == 2, "Must pass a list of 2 states when passing 'initial_states'"
            h_state, c_state = initial_states

        h_list = []
        c_list = []

        for t in range(inputs.shape[1]):
            # LSTM gate steps
            ip = inputs[:, t, :]
            z = self.kernel(ip)
            z += self.recurrent_kernel(h_state)

            z0 = z[:, :self.units]
            # z1 = z[:, self.units: 2 * self.units]
            i = tf.keras.activations.sigmoid(z0)
            z2 = self.recurrent_kernel(h_state*i)[:, 2 * self.units: 3 * self.units]
            # z3 = z[:, 3 * self.units:]

            # gate updates

            # f = tf.keras.activations.sigmoid(z1)
            c = (1-i) * c_state + i * tf.nn.tanh(z2)

            # state updates
            # o = tf.keras.activations.sigmoid(z3)
            h = c #o* tf.nn.tanh(c)

            h_state = h
            c_state = c

            h_list.append(h_state)
            c_list.append(c_state)

        hidden_outputs = tf.stack(h_list, axis=1)
        hidden_states = tf.stack(c_list, axis=1)

        if self.return_states and self.return_sequence:
            return hidden_outputs, [hidden_outputs, hidden_states]
        elif self.return_states and not self.return_sequence:
            return hidden_outputs[:, -1, :], [h_state, c_state]
        elif self.return_sequence and not self.return_states:
            return hidden_outputs
        else:
            return hidden_outputs[:, -1, :]

class BasicLSTMModel(tf.keras.Model):
    def __init__(self, units, num_classes):
        super(BasicLSTMModel, self).__init__()
        self.units = units
        self.lstm = BasicLSTM(units)
        self.classifier = tf.keras.layers.Dense(num_classes)

    def call(self, inputs, training=None, mask=None):
        h = self.lstm(inputs)
        output = self.classifier(h)

        # softmax op does not exist on the gpu, so always use cpu
        with tf.device('/cpu:0'):
            output = tf.nn.softmax(output)

        return output



device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'

with tf.device(device):
    # build model and optimizer
    model = BasicLSTMModel(units, num_classes)
    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()
    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model
    # dummy_x = tf.zeros([1, 28, 28])
    #
    # model._set_inputs(dummy_x)

    # train
    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,
              validation_data=(x_test, y_test_ohe), verbose=1)
    X=model.history.history['acc']
    Y=model.history.history['val_acc']
    plt.plot(X, Y, label='Train')
    plt.xlabel('Training accuracy per epoch')
    plt.ylabel('Test accuracy per epoch')
    # evaluate on test set
    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)
    print("Final test loss and accuracy :", scores)

    saver = tfe.Saver(model.variables)
    saver.save('weights/06_02_rnn/weights.ckpt')



# In[ ]:




